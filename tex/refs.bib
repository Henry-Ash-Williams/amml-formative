@article{baur2021autoencoders,
  title    = {Autoencoders for unsupervised anomaly segmentation in brain MR images: A comparative study},
  journal  = {Medical Image Analysis},
  volume   = {69},
  pages    = {101952},
  year     = {2021},
  issn     = {1361-8415},
  doi      = {https://doi.org/10.1016/j.media.2020.101952},
  url      = {https://www.sciencedirect.com/science/article/pii/S1361841520303169},
  author   = {Christoph Baur and Stefan Denner and Benedikt Wiestler and Nassir Navab and Shadi Albarqouni},
  keywords = {Anomaly segmentation, Detection, Unsupervised, Brain MRI, Autoencoder, Variational, Adversarial, Generative, VAE-GAN, VAEGAN},
  abstract = {Deep unsupervised representation learning has recently led to new approaches in the field of Unsupervised Anomaly Detection (UAD) in brain MRI. The main principle behind these works is to learn a model of normal anatomy by learning to compress and recover healthy data. This allows to spot abnormal structures from erroneous recoveries of compressed, potentially anomalous samples. The concept is of great interest to the medical image analysis community as it i) relieves from the need of vast amounts of manually segmented training data—a necessity for and pitfall of current supervised Deep Learning—and ii) theoretically allows to detect arbitrary, even rare pathologies which supervised approaches might fail to find. To date, the experimental design of most works hinders a valid comparison, because i) they are evaluated against different datasets and different pathologies, ii) use different image resolutions and iii) different model architectures with varying complexity. The intent of this work is to establish comparability among recent methods by utilizing a single architecture, a single resolution and the same dataset(s). Besides providing a ranking of the methods, we also try to answer questions like i) how many healthy training subjects are needed to model normality and ii) if the reviewed approaches are also sensitive to domain shift. Further, we identify open challenges and provide suggestions for future community efforts and research directions.}
}

@misc{byerly2021routingneededcapsules,
  title         = {No Routing Needed Between Capsules},
  author        = {Adam Byerly and Tatiana Kalganova and Ian Dear},
  year          = {2021},
  eprint        = {2001.09136},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2001.09136}
}
@article{chamin2022end,
  author   = {Chamain, Lahiru D. and Qi, Siyu and Ding, Zhi},
  journal  = {IEEE Internet of Things Journal},
  title    = {End-to-End Image Classification and Compression With Variational Autoencoders},
  year     = {2022},
  volume   = {9},
  number   = {21},
  pages    = {21916-21931},
  keywords = {Image coding;Image reconstruction;Codecs;Internet of Things;Decoding;Transform coding;Task analysis;Classification;compression;end-to-end;reconstruction;variational autoencoders (VAEs)},
  doi      = {10.1109/JIOT.2022.3182313}
}

@online{ghassen2017tutorial,
  author       = {Ghassen Hamrouni},
  title        = {Spatial Transformer Networks Tutorial},
  year         = {2017},
  publisher    = {Github},
  howpublished = {Github Repository},
  url          = {https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html}
}

@misc{jaderberg2016spatialtransformernetworks,
  title         = {Spatial Transformer Networks},
  author        = {Max Jaderberg and Karen Simonyan and Andrew Zisserman and Koray Kavukcuoglu},
  year          = {2016},
  eprint        = {1506.02025},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1506.02025}
}

@misc{jang2017categoricalreparameterizationgumbelsoftmax,
  title         = {Categorical Reparameterization with Gumbel-Softmax},
  author        = {Eric Jang and Shixiang Gu and Ben Poole},
  year          = {2017},
  eprint        = {1611.01144},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1611.01144}
}
@misc{kingma2022autoencodingvariationalbayes,
  title         = {Auto-Encoding Variational Bayes},
  author        = {Diederik P Kingma and Max Welling},
  year          = {2022},
  eprint        = {1312.6114},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1312.6114}
}
@article{kullback1951information,
  author    = {S. Kullback and R. A. Leibler},
  title     = {{On Information and Sufficiency}},
  volume    = {22},
  journal   = {The Annals of Mathematical Statistics},
  number    = {1},
  publisher = {Institute of Mathematical Statistics},
  pages     = {79 -- 86},
  year      = {1951},
  doi       = {10.1214/aoms/1177729694},
  url       = {https://doi.org/10.1214/aoms/1177729694}
}
@inproceedings{liu2015faceattributes,
  title     = {Deep Learning Face Attributes in the Wild},
  author    = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month     = {December},
  year      = {2015}
}
@book{prince2023understanding,
  author    = {Simon J.D. Prince},
  title     = {Understanding Deep Learning},
  publisher = {The MIT Press},
  year      = 2023,
  url       = {http://udlbook.com}
}
@inbook{rumelhart1986learning,
  author    = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
  title     = {Learning internal representations by error propagation},
  year      = {1986},
  isbn      = {026268053X},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
  pages     = {318–362},
  numpages  = {45}
}

@article{singh2020investigation,
  title    = {Investigating the impact of data normalization on classification performance},
  journal  = {Applied Soft Computing},
  volume   = {97},
  pages    = {105524},
  year     = {2020},
  issn     = {1568-4946},
  doi      = {https://doi.org/10.1016/j.asoc.2019.105524},
  url      = {https://www.sciencedirect.com/science/article/pii/S1568494619302947},
  author   = {Dalwinder Singh and Birmohan Singh},
  keywords = {Ant lion optimization, Data normalization, Feature selection, Feature weighting, -NN classifier},
  abstract = {Data normalization is one of the pre-processing approaches where the data is either scaled or transformed to make an equal contribution of each feature. The success of machine learning algorithms depends upon the quality of the data to obtain a generalized predictive model of the classification problem. The importance of data normalization for improving data quality and subsequently the performance of machine learning algorithms has been presented in many studies. But, the work lacks for the feature selection and feature weighting approaches, a current research trend in machine learning for improving performance. Therefore, this study aims to investigate the impact of fourteen data normalization methods on classification performance considering full feature set, feature selection, and feature weighting. In this paper, we also present a modified Ant Lion optimization that search feature subsets and the best feature weights along with the parameter of Nearest Neighbor Classifier. Experiments are performed on 21 publicly available real and synthetic datasets, and results are analyzed based on the accuracy, the percentage of feature reduced and runtime. It has been observed from the results that no single method outperforms others. Therefore, we have suggested a set of the best and the worst methods combining the normalization procedure and empirical analysis of results. The better performers are z-Score and Pareto Scaling for the full feature set and feature selection, and tanh and its variant for feature weighting. The worst performers are Mean Centered, Variable Stability Scaling and Median and Median Absolute Deviation methods along with un-normalized data.}
}
@misc{sønderby2015recurrent,
  title         = {Recurrent Spatial Transformer Networks},
  author        = {Søren Kaae Sønderby and Casper Kaae Sønderby and Lars Maaløe and Ole Winther},
  year          = {2015},
  eprint        = {1509.05329},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1509.05329}
}
@misc{subramanian2020,
  author       = {Subramanian, A.K},
  title        = {PyTorch-VAE},
  year         = {2020},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {https://github.com/AntixK/PyTorch-VAE}
}

@misc{vahdat2021nvaedeephierarchicalvariational,
  title         = {NVAE: A Deep Hierarchical Variational Autoencoder},
  author        = {Arash Vahdat and Jan Kautz},
  year          = {2021},
  eprint        = {2007.03898},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/2007.03898}
}
@online{xiao2017fashion,
  author      = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title       = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date        = {2017-08-28},
  year        = {2017},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  eprint      = {cs.LG/1708.07747}
}